# INFERNEON ML API USAGE
This document describes the details of each project in the Inferneon ML API and how to integrate and use each of the modules in your project. For the purpose of illustration, we consider the following data which represents a canonical example of categorical values indicating whether or not a player would play tennis depending upon weather conditions. A few records are shown below (Example 1):
**_Outlook,Temperature,Humidity,Wind,PlayTennis_**
_Sunny,Hot,High,Weak,No
Sunny,Hot,High,Strong,No
Overcast,Hot,High,Weak,Yes
Rain,Mild,High,Weak,Yes
Rain,Cool,Normal,Weak,Yes_

Each column in the data above represents a feature or attribute. The last column can be considered the target or class feature that several machine learning algorithms aim to predict based on models generated by the training phase of a supervised learning algorithm. In this specific case, one goal would be to predict if a player – or to categorically determine –would play tennis depending upon the weather conditions as given by the other features. 
The data might also include a mix of both categorical and continuous values. For example, instead of categorical values like “Hot”, “Mild” or “Cool” for the Temperature feature, data might have been collected in Fahrenheit (Example 2):
Outlook, Temperature, Humidity, Wind, PlayTennis
Sunny, 105.6, High, Weak, No
Sunny, 103.4, High, Strong, No
Overcast, 104.8, High, Weak, Yes
Rain, 90.4, High, Weak, Yes
Rain, 56.6, Normal, Weak, Yes

With the above examples in context, a description of the projects in Inferneon ML and how it can be used to integrate into other project follows.
DataUtils 
In many practical applications of machine learning, data is collected either as continuous-valued, categorical values or as a mix of both as shown in the example above. However, Spark MLLib uses the Resilient Distributed Data (RDD) abstraction of a collection of labelled points as input to several of the algorithms in its framework. DataUtils can be used to bridge this gap. It consists of APIs to:
a)	Infer the schema from a given CSV data and also obtain a list of labelled points if a schema could be determined. 
b)	Load a list of labelled points given the CSV data and a schema defining the data.
c)	Load a RDD of labelled points given the CSV data as an RDD and a schema defining the data.
Each of these three primary methods are described below:
Using inferSchema()
The input data is assumed to be in the form of a CSV with a header that describes the name of each column or feature. Data can consist of either categorical or numerical values or a mix of both. The data can also consist of records with missing values. A detailed description of the inferSchema() method can be found here.
Let’s assume the file fullPath consists of data as described in Example 1. The following Scala code calls inferSchema() to learn the schema of the data in the input file specified by fullPath: 
val (errors, schema, points) = DataUtils.inferSchema(fullPath, 4, true)

println("Number of errors: " + errors.size)
println("Schema: ")
schema foreach {case (featureName, values) =>
  print(featureName + ": ")
  println(values mkString(", "))
}
println("Labeled points: ")
points foreach { labeledPoint =>
  println("   " + labeledPoint.toString)
}

The output is shown below:
Number of errors: 0
Schema: 
   Outlook: Sunny, Overcast, Rain
   Temperature: Hot, Mild, Cool
   Humidity: High, Normal
   Wind: Weak, Strong
   PlayTennis: No, Yes
Labeled points: 
   Some((0.0,[0.0,0.0,0.0,0.0]))
   Some((0.0,[0.0,0.0,0.0,1.0]))
   Some((1.0,[1.0,0.0,0.0,0.0]))
   Some((1.0,[2.0,1.0,0.0,0.0]))
   Some((1.0,[2.0,2.0,1.0,0.0])) 

The method returns a 3-tuple that contains: a) any errors encountered while attempting to learn the schema, b) the schema itself and c) a list of labelled points wrapped in an Optional value. As seen in the output, the method did not encounter any errors; the schema was inferred as five attributes with corresponding categorical values. Each element in the list of labelled points wrapped in an Optional value contains the indexes of the occurrence of the categorical value as it appears in the list for that feature. 
For data containing both categorical and numerical values, we see the following output (assuming that content of fullPath consists of data as outlined in Example 2) upon running the same sample code:

Number of errors: 0
Schema: 
   Outlook: Sunny, Overcast, Rain
   Temperature: 
   Humidity: High, Normal
   Wind: Weak, Strong
   PlayTennis: No, Yes
Labeled points: 
   Some((0.0,[0.0,105.6,0.0,0.0]))
   Some((0.0,[0.0,103.4,0.0,1.0]))
   Some((1.0,[1.0,104.8,0.0,0.0]))
   Some((1.0,[2.0,90.4,0.0,0.0]))
   Some((1.0,[2.0,56.6,1.0,0.0]))

In this case, it can be seen that the Temperature feature has no categorical values since it was inferred as a numerical (real) value. In such cases, the actual value of the number is part of the labelled point.
Using loadLabeledPoints()
This method can be used to load a list of labelled points given the CSV data with categorical or/and real-valued data and a schema to define the data. A complete description of the interface and usage can be found here.
The following code sample demonstrates how to use this method:
// Define the schema
val schema: Array[(String, Array[String])] = Array[(String, Array[String])](("Outlook", Array("Sunny", "Overcast", "Rain")),
  ("Temperature", Array("Hot", "Mild", "Cool")),
  ("Humidity", Array("High", "Normal")),
  ("Wind", Array("Weak", "Strong")),
  ("PlayTennis", Array("No", "Yes")))

// Call loadLabeledPoints() specifying the input file and the schema
val (errors, points) = DataUtils.loadLabeledPoints(fullPath, schema, 4, true)
println("Number of errors: " + errors.size)

println("Labeled points: ")
points foreach { labeledPoint =>
  println("   " + labeledPoint.toString)
}


Number of errors: 0
Labeled points: 
   Some((0.0,[0.0,0.0,0.0,0.0]))
   Some((0.0,[0.0,0.0,0.0,1.0]))
   Some((1.0,[1.0,0.0,0.0,0.0]))
   Some((1.0,[2.0,1.0,0.0,0.0]))
   Some((1.0,[2.0,2.0,1.0,0.0]))


If the data contains continuous values like the Temperature feature in Example 2, the corresponding array should be specified as an empty array. An empty array in place of categorical values indicates that the corresponding feature should be treated as real-valued:
// Define the schema for Example 2
val schema: Array[(String, Array[String])] = Array[(String, Array[String])](("Outlook", Array("Sunny", "Overcast", "Rain")),
  ("Temperature", Array.empty,
  ("Humidity", Array("High", "Normal")),
  ("Wind", Array("Weak", "Strong")),
  ("PlayTennis", Array("No", "Yes")))

Using loadLabeledPointsRDD()
This method is similar to the loadLabeledPoints() except that it converts a RDD in CSV format to an RDD of labelled points. The input is therefore an RDD of data in CSV format. The following example loads an RDD of labelled points given an RDD of data in CSV format:
// Define schema
val schema: Array[(String, Array[String])] = Array[(String, Array[String])](("Outlook", Array("Sunny", "Overcast", "Rain")),
  ("Temperature", Array("Hot", "Mild", "Cool")),
  ("Humidity", Array("High", "Normal")),
  ("Wind", Array("Weak", "Strong")),
  ("PlayTennis", Array("No", "Yes")))

// Get the Spark context
val conf = new SparkConf().setMaster("local[1]").setAppName("DataUtilsTest")
val sc = new SparkContext(conf)


// Get an RDD from input file
val rdd = sc.textFile(fullPath, sc.defaultMinPartitions)

// Generate an RDD of labelled points from RDD created above
val (errors, points) = DataUtils.loadLabeledPointsRDD(sc, rdd, schema, 4, true)


BayesNet
The BayesNet project consists of APIs that help learn a Bayesian belief network (or just belief network) from data. The learning involves inference of the structure in the form a directed acyclic graph (DAG) as well as the conditional probability distribution tables associated with each node in the graph.
In the current version, the following algorithms are available:
a.	Hill climbing: Uses the Hill climbing algorithm to learn the Bayesian network. The primary interface that should be used is the HillClimber object.
b.	Simulated annealing: Uses the simulated annealing algorithm to learn the Bayesian network. The primary interface that should be used is the SimulatedAnnealing object.
Both the algorithms use local search methods, i.e., small changes are made at each node and a score is used to check if the score for the overall state has improved. Currently, the scoring is based on entropy computations. 
Both the algorithms have Java-friend APIs that can be used to call from Java code. For the purpose of illustration, consider the following data with “Sales” as the target feature that depends on upon three other features (Example 3) :
IsMetro, MidAge, Gender, Sales
y,y,f,y
n,y,f,y
y,y,f,y
y,y,f,y
y,y,m,y
y,n,m,y
y,y,m,y

The other features are like IsMetro, MidAge, and Gender could be regarded as factors that influence the sale of a specific item. Each record is a transactional data, typical of data in retail businesses. The inference of belief networks for such data will be useful in reasoning about the dynamics of a market.

Using the Hill Climbing algorithm in Scala code
The HillClimber object is the primary interface for the hill climbing algorithm in Scala. The object defines a learnNetwork() method which should be used to learn the Bayesian network. The method accepts the data as RDD, the schema and a few hyper parameters relevant to the algorithm. For Scala code, the input data should include an RDD of labelled points. A detailed description of the method can be found here.
The following example illustrates the use of the learnNetwork() method in Scala code:
val filePath = "/SampleSales.csv"
val fullPath = getClass.getResource(filePath).getFile

// If we do not have a RDD of labelled points, infer the schema using a sample file
val result = DataUtils.inferSchema(fullPath, 3, false)
val schema = result._2
val labeledPoints = result._3

// Get the RDD of labelled points
val rdd: RDD[Option[LabeledPoint]] = sc.parallelize(labeledPoints)

// Get only valid RDD points by discarding records that could not be created
val pointsRDD: RDD[LabeledPoint] = rdd filter {lp => lp.isDefined} map {point => 
point.get}

// Learn the network
val network = HillClimber.learnNetwork(pointsRDD, 2, 0.5, false, 3, schema)
println(network.treeDescription(schema))

Here is a sample output:
sales: gender
sales
|   midAge
|   |   isMetro: gender
sales
|   isMetro: gender
 CONDITIONAL PROBABILITY DISTRIBUTION TABLES:
Table for gender:
 isMetro:y sales:y  => 48.5, 12.5
  isMetro:y sales:n  => 12.5, 28.5
  isMetro:n sales:y  => 19.5, 8.5
  isMetro:n sales:n  => 13.5, 20.5
Table for midAge:
 sales:y  => 60.5, 27.5
  sales:n  => 40.5, 33.5
Table for sales:
 87.5, 73.5
Table for isMetro:
 midAge:y sales:y  => 60.5, 0.5
  midAge:y sales:n  => 40.5, 0.5
  midAge:n sales:y  => 0.5, 27.5
  midAge:n sales:n  => 0.5, 33.5

The DAG should be interpreted as having Sales as a root node with and having Gender, MidAge and IsMetro as it’s children and so on. The probability distribution tables are displayed below the tree. For the Gender feature that has IsMetro and Sales as its parent, we see that the conditional probability distribution table has entries for all possible combination of the parent’s values with corresponding counts (frequencies) of occurrences of each value of Gender (“y” and ”n”). For example, the first entry for that table indicates that for a combination of IsMetro=”y” and Sales=”y”, we have 48.5 occurrences of Gender=”f” and 12.5 of Gender =”m” in the data. The probabilities can be computed using these frequencies by normalizing the terms as the probabilities in each row should add up to 1.0.
Using HillClimber in Java code
The learnNetwork method has a couple of other versions that can be called from Java code. Essentially, the overloaded methods provide a means to pass JavaRDD<LabeledPoint> or JavaRDD<String>. The network is returned in a BayesianBeliefNetwork object that has a method to retrieve a corresponding Java-friendly object, JavaBayesianBeliefNetwork . Here is an example of using learnNetwork() from Java code:

// Get a JavaRDD of labelled points
JavaRDD<org.apache.spark.mllib.regression.LabeledPoint> rddLabeledPoints = 
				……………………….

// Create the schema using Java data structures.
List<Map<String, List<String>>> schema = getSchema()

// Learn the network
BayesianBeliefNetwork nw = SimulatedAnnealing.learnNetwork(rddLabeledPoints, 
				maxNumberOfParents, prior, false, classIndex, format, 
				initTemperature, maxIterations, temperatureStep, ScoringType.ENTROPY());
		System.out.println(nw.treeDescription(format));

		JavaBayesianBeliefNetwork network = BayesianBeliefNetwork.getJavaBayesianBeliefNetwork(nw);
List<Node> nodes = javaBayesianBeliefNetwork.nodes();
Set<Edge> edges = javaBayesianBeliefNetwork.edges();
Map<Object, JavaCPT> cpts = javaBayesianBeliefNetwork.cpts();
As seen in the code above, the Java-friendly interface has data structures that can be used in Java code. Refer to the code comments for more information on the JavaCPT object that represents a conditional probability table.
Using SimulatedAnnealing in Scala code
The interface for the simulated annealing algorithm is similar to the ones used for the hill climbing algorithm. The difference only lies in the hyper parameters that need to be passed to the corresponding learnNetwork() method to learn the Bayesian network. This method is also overloaded to provide a Java-friendly interface.
Here is a sample Scala code to use SimulatedAnnealing:
val filePath = "/SampleSales.csv"
val fullPath = getClass.getResource(filePath).getFile

// If we do not have a RDD of labelled points, infer the schema using a sample file
val result = DataUtils.inferSchema(fullPath, 3, false)
val schema = result._2
val labeledPoints = result._3

// Get the RDD of labelled points
val rdd: RDD[Option[LabeledPoint]] = sc.parallelize(labeledPoints)

// Get only valid RDD points by discarding records that could not be created
val pointsRDD: RDD[LabeledPoint] = rdd filter {lp => lp.isDefined} map {point => 
point.get}

// Learn the network
val network = SimulatedAnnealing.learnNetwork(pointsRDD, 2, 0.5, false, 3, schema, 10.0, 300, 0.999)

println(network.treeDescription(schema))

The above example prints out the same tree as that shown for the HillClimber example. 
The Java-friendly API for usage in Java code is similar to the ones defined for the HillClimber.






